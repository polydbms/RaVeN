package benchi

import edu.ucr.cs.bdlab.beast.geolite.{IFeature, ITile}
import edu.ucr.cs.bdlab.beast.{RasterReadMixinFunctions, ReadWriteMixinFunctions}
import edu.ucr.cs.bdlab.raptor.RaptorJoinFeature
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, SparkSession}

/**
 * Scala examples for Beast
 */
object RaptorScala {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Raptor Scala")
    // Set Spark master to local if not already set
    if (!conf.contains("spark.master"))
      conf.setMaster("local[*]")

    val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
    import spark.implicits._

    val sc = spark.sparkContext
    try {
      val time_delta = System.currentTimeMillis() * scala.math.pow(10, 6) - System.nanoTime()
      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,start,execution,beast,vector,inner")

      // 1- Load raster and vector data
      val vector: RDD[IFeature] = sc.spatialFile("{{vector_path}}")
      {% for c in vector_conditions %}
        .filter(f => f.getAs[{{c.datatype}}]("{{c.field}}") {{c.condition}})
      {% endfor %}
      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,end,execution,beast,vector,inner")

      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,start,execution,beast,raster,inner")
      val raster: RDD[ITile[{{raster.datatype}}]] = sc.geoTiff[{{raster.datatype}}]("{{raster_geotiff_path}}")

      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,end,execution,beast,raster,inner")

      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,start,execution,beast,,inner")

      val join: RDD[RaptorJoinFeature[{{raster.datatype}}]] = raster.raptorJoin(vector)
      {% for c in raster_conditions %}
        .filter(v => v.m {{c.condition}})
      {% endfor %}

      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,mid,execution,beast,,inner")

      val rdd_raw: RDD[({{get.datatype}}, {{raster.datatype}})] = join.map(v => (v.feature.getAs[{{get.datatype}}]("{{get.field}}"), v.m))

      spark.createDataset(rdd_raw).toDF(Seq("{{get.field}}", "{{raster.field}}"): _*).createOrReplaceTempView("df_raw")
      val zonal_stats: DataFrame = spark.sql("{{sql_query}}")

      zonal_stats.coalesce(1).write.option("header",true).csv("/data/beast_result")

      println(f"benchi_marker,${(System.nanoTime() + time_delta) * scala.math.pow(10, -9)}%.9f,end,execution,beast,,inner")
    } finally {
      spark.stop()
    }
  }
}
